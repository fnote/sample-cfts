AWSTemplateFormatVersion: 2010-09-09
Description: "Reference Price Data Populator for Prize Zone data"
Parameters:
  LambdaRole:
    Description: ARN of the iam role for the lambda function
    Type: String
    Default: arn:aws:iam::037295147636:role/CP-REF-ETLTriggerLambaRole-DEV
  StepFuctionExecutionRole:
    Description: ARN of the iam role for the step function
    Type: String
    Default: arn:aws:iam::037295147636:role/CP-REF-ETLStepFunctionExecutionRole-DEV
  GlueJobExecutionRole:
    Description: ARN of the iam role for the glue job
    Type: String
    Default: arn:aws:iam::037295147636:role/CP-REF-ETLGlueRole-DEV
  GlueJobDataBackupRole:
    Description: ARN of the iam role for the glue job to backup data
    Type: String
    Default: arn:aws:iam::037295147636:role/CP-REF-ETLDataBackupGlueRole-DEV
  IntermediateStorage:
    Description: S3 bucket name to store intermediate files
    Type: String
    Default: cp-ref-etl-output-bucket-dev
  ETLDataBackUpStorage:
    Description: S3 bucket to archive files
    Type: String
  EnvironmentShort:
    Description: Environment for application
    Type: String
    Default: DEV
    AllowedValues:
      - DEV
      - QA
      - STG
      - EXE
      - TST
      - PROD
    ConstraintDescription: Must be a valid environment
  DatabaseConnection:
    Description: Name of the databse connection
    Type: String
    Default: cp-ref-etl-common-connection-DEV
  NotifierLambdaArn:
    Description: Arn of the notifier Lambda
    Type: String
    Default: arn:aws:lambda:us-east-1:037295147636:function:CP-REF-etl-notifier-DEV
  ETLMaximumConcurrency:
    Description: Maximum Concurrency Allowed for Price Zone Glue jobs
    Type: String
    Default: 5
  LoadJobMaximumConcurrency:
    Description: Maximum Concurrency Allowed for Price Zone load jobs
    Type: String
    Default: 10
  TransformJobMaxDPUCount:
    Description: Maximum DPU count allocated for Transform spark job
    Type: String
    Default: 100
  TransformJobMinDPUCount:
    Description: Minimum DPU count allocated for Transform spark job
    Type: String
    Default: 5
  ActiveOpcos:
    Description: Active Opcos
    Type: String
    Default: '019'
  PriceZoneFilePartialLoadPrefixes:
    Description: List of prefixes for price zone file partial load prefixes
    Type: String
    Default: ctt,itt
  TransformJobWorkerType:
    Description: AWS Glue worker type for Spark job
    Type: String
    Default: Standard
    AllowedValues:
      - Standard
  PONumber:
    Description: PO Number for billing
    Type: String
    Default: '7000002358'
    MinLength: '1'
    MaxLength: '255'
    AllowedPattern: "[\\x20-\\x7E]*"
    ConstraintDescription: Must contain only ASCII characters.
  ApplicationID:
    Description: Application ID - Official Application_ID, this is generated by Sysco's
      CMDB
    Type: String
    Default: APP-001151
    MinLength: '1'
    MaxLength: '255'
    AllowedPattern: "[\\x20-\\x7E]*"
    ConstraintDescription: Must contain only ASCII characters.
  ApplicationName:
    Description: Application_Name - Common, user-friendly name
    Type: String
    Default: Cloud Pricing
    MinLength: '1'
    MaxLength: '255'
    AllowedPattern: "[\\x20-\\x7E]*"
    ConstraintDescription: Must contain only ASCII characters.
  Approver:
    Description: Person approving instance funding.  This should be Email address
      formatted
    Type: String
    Default: villanueva.loi@corp.sysco.com
    MinLength: '1'
    MaxLength: '255'
  Owner:
    Description: Email address usually Product/ Platform Owner, though team distribution
      list for technical product/platform team contact
    Type: String
    Default: krishan.senevirathne@sysco.com
    MinLength: '1'
    MaxLength: '255'
  Component:
    Description: Component name
    Type: String
    Default: Reference Pricing Api
  SupportEmail:
    Description: Email distribution list for technical product/platform team contact
    Type: String
    Default: 000-BT-PricingPlatform@Corp.sysco.com
    MinLength: '1'
    MaxLength: '255'
  ProjectID:
    Description: Project_ID - BT Project ID for this workload
    Type: String
    Default: BT.001176
    MinLength: '1'
    MaxLength: '255'
    AllowedPattern: "[\\x20-\\x7E]*"
    ConstraintDescription: Must contain only ASCII characters.
  2WTAGGER:
    Description: Used by 2nd Watch Managed Services in shared accounts to determine
      if a resource is supported
    Type: String
    Default: team-managed
    MinLength: '1'
    MaxLength: '255'
    AllowedValues:
      - team-managed
      - adlm-managed
      - 2w-managed
    ConstraintDescription: Must contain only ASCII characters.
  Platform:
    Description: Platform
    Type: String
    Default: Cloud Pricing V4

Mappings:
  EnvMap:
    DEV:
      val: dev
      name: Development
    QA:
      val: qa
      name: Quality
    STG:
      val: stg
      name: Staging
    EXE:
      val: exe
      name: Execution
    TST:
      val: tst
      name: Tuning
    PROD:
      val: prod
      name: Production

Resources:
  PriceZoneStorage:
    Type: AWS::S3::Bucket
    DependsOn: LambdaInvokePermission
    Properties:
      BucketName: !Sub
        - 'cp-ref-etl-price-zone-storage-${env}'
        - { env: !FindInMap [ EnvMap, !Ref EnvironmentShort, val ]}
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt PriceZoneInputTrigger.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: gz
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component

  PrizeZoneSparkJobLogStorage:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub
        - 'cp-ref-etl-prize-zone-spark-temp-dir-${env}'
        - { env: !FindInMap [ EnvMap, !Ref EnvironmentShort, val ]}
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component

  PriceZoneETLMaximumConcurrency:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Join ['', ['/CP/', !Ref EnvironmentShort, '/ETL/REF_PRICE/PRICE_ZONE/MAX_CONCURRENCY']]
      Type: String
      Value: !Ref ETLMaximumConcurrency
      Tier: Standard
      Description: Maximum concurrency allowed for Glue jobs

  PriceZoneETLWorkerType:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Join ['', ['/CP/', !Ref EnvironmentShort, '/ETL/REF_PRICE/PRICE_ZONE/WORKER_TYPE']]
      Type: String
      Value: !Ref TransformJobWorkerType
      Tier: Standard
      Description: Worker type for ETL Transform job

  PriceZoneETLMaxWorkerCount:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Join ['', ['/CP/', !Ref EnvironmentShort, '/ETL/REF_PRICE/PRICE_ZONE/WORKER_COUNT/MAX']]
      Type: String
      Value: !Ref TransformJobMaxDPUCount
      Tier: Standard
      Description: DPU count for ETL spark job for Full load

  PriceZoneETLMinWorkerCount:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Join ['', ['/CP/', !Ref EnvironmentShort, '/ETL/REF_PRICE/PRICE_ZONE/WORKER_COUNT/MIN']]
      Type: String
      Value: !Ref TransformJobMinDPUCount
      Tier: Standard
      Description: DPU count for ETL spark job for New customer

  PriceZoneETLActiveOpcos:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Join ['', ['/CP/', !Ref EnvironmentShort, '/ETL/REF_PRICE/PRICE_ZONE/ACTIVE/BUSINESS/UNITS']]
      Type: String
      Value: !Ref ActiveOpcos
      Description: Active opcos available for prize zone

  PriceZoneETLPartialLoadPrefixes:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Join ['', ['/CP/', !Ref EnvironmentShort, '/ETL/REF_PRICE/PRICE_ZONE/PARTIAL_LOAD_PREFIXES']]
      Type: String
      Value: !Ref PriceZoneFilePartialLoadPrefixes
      Tier: Standard
      Description: List of prefixes for price zone file partial load prefixes

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt PriceZoneInputTrigger.Arn
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub
        - 'arn:aws:s3:::cp-ref-etl-price-zone-storage-${env}'
        - { env: !FindInMap [ EnvMap, !Ref EnvironmentShort, val ]}

  PriceZoneInputTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda function that listens to price zone data file uploads and triggers the step function
      FunctionName: !Sub 'CP-REF-etl-price-zone-trigger-${EnvironmentShort}'
      Runtime: python3.8
      Role: !Ref LambdaRole
      Handler: s3_trigger_lambda.lambda_handler
      Code:
        S3Bucket: sysco-us-east-1-prcp-nonprod-codedeploy
        S3Key: !Sub 'ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/s3_trigger_lambda.zip'
      Environment:
        Variables:
          stepFunctionArn: !Ref PriceZoneStateMachine
          intermediateStorageS3: !Ref IntermediateStorage
          env: !Ref EnvironmentShort
      Timeout: 60
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component

  PrizeZoneDecompressJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: !Sub 's3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/decompress_job.py'
      DefaultArguments:
        "--job-bookmark-option": "job-bookmark-disable"
        "--extra-py-files": !Sub "s3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/smart_open-2.1.0-py3.6.egg"
      ExecutionProperty:
        MaxConcurrentRuns: !Ref ETLMaximumConcurrency
      GlueVersion: 1.0
      MaxRetries: 0
      MaxCapacity: 1.0
      Timeout: 4320
      Name: !Sub 'CP-REF-etl-prize-zone-decompression-job-${EnvironmentShort}'
      Role: !Ref GlueJobExecutionRole
      Tags: {
        'Technical:ApplicationName': !Ref ApplicationName,
        'Technical:ApplicationID': !Ref ApplicationID,
        'Technical:PlatformOwner': !Ref Owner,
        'Technical:Environment': !FindInMap [ EnvMap, !Ref EnvironmentShort, name ],
        'Support_Email': !Ref SupportEmail,
        'Approver': !Ref Approver,
        'PO_Number': !Ref PONumber,
        'Project_ID': !Ref ProjectID,
        '2WTAGGER': !Ref 2WTAGGER,
        'Platform': !Ref Platform,
        'Component': !Ref Component
      }

  PrizeZoneTransformJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: glueetl
        PythonVersion: 3
        ScriptLocation: !Sub s3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/transform_spark_job.py
      DefaultArguments:
        "--extra-py-files": !Sub "s3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/validator.py,s3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/constants.py"
        "--enable-metrics": ""
        "--enable-continuous-cloudwatch-log": "true"
        "--enable-continuous-log-filter": "true"
        "--enable-spark-ui": 'true'
        "--spark-event-logs-path": !Sub "s3://${PrizeZoneSparkJobLogStorage}"
        "--TempDir": !Sub "s3://${PrizeZoneSparkJobLogStorage}"
        "--METADATA_LAMBDA": !Sub 'CP-REF-PRICE-etl-metadata-aggregator-${EnvironmentShort}'
      ExecutionProperty:
        MaxConcurrentRuns: !Ref ETLMaximumConcurrency
      GlueVersion: 2.0
      WorkerType: !Ref TransformJobWorkerType
      NumberOfWorkers: !Ref TransformJobMaxDPUCount
      MaxRetries: 0
      Name: !Sub 'CP-REF-etl-prize-zone-transform-job-${EnvironmentShort}'
      Role: !Ref GlueJobExecutionRole
      Tags: {
        'Technical:ApplicationName': !Ref ApplicationName,
        'Technical:ApplicationID': !Ref ApplicationID,
        'Technical:PlatformOwner': !Ref Owner,
        'Technical:Environment': !FindInMap [ EnvMap, !Ref EnvironmentShort, name ],
        'Support_Email': !Ref SupportEmail,
        'Approver': !Ref Approver,
        'PO_Number': !Ref PONumber,
        'Project_ID': !Ref ProjectID,
        '2WTAGGER': !Ref 2WTAGGER,
        'Platform': !Ref Platform,
        'Component': !Ref Component
      }

  OpCoFilesFetchLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda function that fetches the list of OpCo wise partitioned files
      FunctionName: !Sub 'CP-REF-etl-price-zone-opco-files-fetch-${EnvironmentShort}'
      Runtime: python3.7
      Role: !Ref LambdaRole
      Handler: index.lambda_handler
      Code:
        ZipFile: |
          import boto3
          import re

          def extract_opco_id(x):
               p = re.search('opco_id=(\d+?)/', x['Key'])
               return p and p.group(1)
          def lambda_handler(event, context):
              client = boto3.client('s3')

              paginator = client.get_paginator('list_objects_v2')
              pages = paginator.paginate(Bucket=event['intermediate_s3_name'], Prefix=event['partitioned_files_key'])

              opco_id_set = set()
              for page in pages:
                  opco_ids = map(extract_opco_id, page['Contents'])
                  for opco_id in set(opco_ids):
                      opco_id_set.add(opco_id)

              print(list(opco_id_set))
              return list(opco_id_set)
      Timeout: 180
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component

  PriceZoneJobStatusAnalyzerLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda function that analyzes status of previous job runs
      FunctionName: !Sub 'CP-REF-etl-price-zone-job-status-analyzer-${EnvironmentShort}'
      Runtime: python3.7
      Role: !Ref LambdaRole
      Handler: index.lambda_handler
      Code:
        ZipFile: |
          def lambda_handler(event, context):

              jobStatuses = event['loadJobStatuses']

              isAllSuccess = True
              for jobStatus in jobStatuses:
                  if 'loadJob' in jobStatus:
                      if 'JobRunState' in jobStatus['loadJob']:
                          if jobStatus['loadJob']['JobRunState'] != "SUCCEEDED":
                              isAllSuccess = False
                              break
                  else:
                      isAllSuccess = False
                      break
              return {
                  'isAllSuccess': isAllSuccess
                    }
      Timeout: 60
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component

  PriceZoneETLWaitStatusAnalyzerLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda function that checks the execution status of a given step function
      FunctionName: !Sub 'CP-REF-etl-price-zone-wait-status-analyzer-${EnvironmentShort}'
      Runtime: python3.8
      Role: !Ref LambdaRole
      Handler: analyze_etl_wait_status.lambda_handler
      Code:
        S3Bucket: sysco-us-east-1-prcp-nonprod-codedeploy
        S3Key: !Sub 'ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/analyze_etl_wait_status.zip'
      Environment:
        Variables:
          intermediateStorageS3: !Ref IntermediateStorage
          env: !Ref EnvironmentShort
      Timeout: 60
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component

  PrizeZoneDataLoadJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: !Sub 's3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/load_job.py'
      DefaultArguments:
        "--extra-py-files": !Sub 's3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/libraries/SQLAlchemy-1.3.17-cp35-cp35m-manylinux2010_x86_64.whl,3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/libraries/PyMySQL-0.9.3-py2.py3-none-any.whl'
        "--GLUE_CONNECTION_NAME": !Ref DatabaseConnection
        "--METADATA_LAMBDA": !Sub 'CP-REF-PRICE-etl-metadata-aggregator-${EnvironmentShort}'
      ExecutionProperty:
        MaxConcurrentRuns: !Ref LoadJobMaximumConcurrency
      GlueVersion: 1.0
      MaxRetries: 0
      MaxCapacity: 1.0
      Connections:
        Connections:
          - !Ref DatabaseConnection
      Name: !Sub 'CP-REF-etl-prize-zone-load-job-${EnvironmentShort}'
      Role: !Ref GlueJobExecutionRole
      Tags: {
        'Technical:ApplicationName': !Ref ApplicationName,
        'Technical:ApplicationID': !Ref ApplicationID,
        'Technical:PlatformOwner': !Ref Owner,
        'Technical:Environment': !FindInMap [ EnvMap, !Ref EnvironmentShort, name ],
        'Support_Email': !Ref SupportEmail,
        'Approver': !Ref Approver,
        'PO_Number': !Ref PONumber,
        'Project_ID': !Ref ProjectID,
        '2WTAGGER': !Ref 2WTAGGER,
        'Platform': !Ref Platform,
        'Component': !Ref Component
      }

  PrizeZoneDataBackupJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: !Sub 's3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/price_zone/data_backup_job.py'
      DefaultArguments:
        "--INTERMEDIATE_S3_BUCKET": !Ref IntermediateStorage
        "--ARCHIVING_S3_BUCKET": !Ref ETLDataBackUpStorage
        "--extra-py-files": !Sub "s3://sysco-us-east-1-prcp-nonprod-codedeploy/ReferencePricingApi/DataPopulator/${EnvironmentShort}/common_scripts/move_s3_objects.py"
      ExecutionProperty:
        MaxConcurrentRuns: !Ref ETLMaximumConcurrency
      GlueVersion: 1.0
      MaxRetries: 0
      MaxCapacity: 1.0
      Name: !Sub 'CP-REF-etl-prize-zone-backup-job-${EnvironmentShort}'
      Role: !Ref GlueJobDataBackupRole
      Tags: {
        'Technical:ApplicationName': !Ref ApplicationName,
        'Technical:ApplicationID': !Ref ApplicationID,
        'Technical:PlatformOwner': !Ref Owner,
        'Technical:Environment': !FindInMap [ EnvMap, !Ref EnvironmentShort, name ],
        'Support_Email': !Ref SupportEmail,
        'Approver': !Ref Approver,
        'PO_Number': !Ref PONumber,
        'Project_ID': !Ref ProjectID,
        '2WTAGGER': !Ref 2WTAGGER,
        'Platform': !Ref Platform,
        'Component': !Ref Component
      }

  PriceZoneStepFunctionExecutionLogs:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub 'cp-ref-etl-prize-zone-step-function-logs-${EnvironmentShort}'
      RetentionInDays: 30

  PriceZoneStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub 'CP-REF-etl-prize-zone-state-machine-${EnvironmentShort}'
      StateMachineType: STANDARD
      DefinitionString: !Sub
        - |-
          {
            "Comment": "Transform and Loads EATs data from S3 to DB",
            "StartAt": "Analyze wait status",
            "States": {
            "Analyze wait status": {
               "Type": "Task",
               "Resource": "arn:aws:states:::lambda:invoke",
               "Parameters": {
                 "FunctionName": "${waitStatusAnalyzerLambdaArn}",
                 "Payload": {
                   "stepFunctionExecutionId.$": "$$.Execution.Id",
                   "stepFunctionArn.$": "$$.StateMachine.Id"
                 }
               },
               "ResultPath": "$.waitStatus",
               "Catch": [
                 {
                   "ErrorEquals": [
                     "States.TaskFailed"
                   ],
                   "ResultPath": "$.error",
                   "Next": "Notify Failure"
                 }
               ],
               "Next": "WaitOrContinue"
             },
             "WaitOrContinue": {
               "Comment": "Check on wait status and wait or continue ETL",
               "Type": "Choice",
               "Choices": [
                 {
                   "Variable": "$.waitStatus.Payload.shouldWait",
                   "BooleanEquals": true,
                   "Next": "waitExecution"
                 }
               ],
               "Default": "Decompress"
             },
             "waitExecution": {
               "Type": "Wait",
               "Seconds": 300,
               "Next": "Analyze wait status"
             },
              "Decompress": {
                "Comment": "Decompress python shell job",
                "Type": "Task",
                "Resource": "arn:aws:states:::glue:startJobRun.sync",
                  "Parameters": {
                  "JobName": "${prizeZoneDecompressJobName}",
                  "Arguments": {
                    "--s3_path.$": "$.s3_path",
                    "--decompressed_file_path.$": "$.decompressed_file_path"
                  }
                },
                "ResultPath": "$.response",
                "Catch": [
                  {
                    "ErrorEquals": [
                      "States.TaskFailed"
                    ],
                    "ResultPath": "$.error",
                    "Next": "Notify Failure"
                  }
                ],
                "Next": "Transform"
              },
              "Transform": {
                "Comment": "Transform spark job",
                "Type": "Task",
                "Resource": "arn:aws:states:::glue:startJobRun.sync",
                  "Parameters": {
                  "JobName": "${prizeZoneTransformJobName}",
                  "Arguments": {
                    "--decompressed_file_path.$": "$.decompressed_file_path",
                    "--partitioned_files_path.$": "$.partitioned_files_path",
                    "--enable-continuous-cloudwatch-log": "true",
                    "--active_opcos.$": "$.active_opcos",
                    "--intermediate_s3_name.$": "$.intermediate_s3_name",
                    "--intermediate_directory_path.$": "$.intermediate_directory_path",
                    "--metadata_aggregator": "${metadataAggregator}"
                  },
                  "NumberOfWorkers.$": "$.worker_count",
                  "WorkerType.$": "$.worker_type"
                },
                "ResultPath": "$.response",
                "Catch": [
                  {
                    "ErrorEquals": [
                      "States.TaskFailed"
                    ],
                    "ResultPath": "$.error",
                    "Next": "Notify Failure"
                  }
                ],
                "Next": "Fetch File List"
              },
              "Fetch File List": {
                "Type": "Task",
                "Resource": "arn:aws:states:::lambda:invoke",
                "Parameters": {
                  "FunctionName": "${opCoFilesFetchLambdaArn}",
                  "Payload":{
                     "intermediate_s3_name.$":"$.intermediate_s3_name",
                     "partitioned_files_key.$":"$.partitioned_files_key"
                  }
                },
                "ResultPath": "$.opcoList",
                "Catch": [
                  {
                    "ErrorEquals": [
                      "States.TaskFailed"
                    ],
                    "ResultPath": "$.error",
                    "Next": "Notify Failure"
                  }
                ],
                "Next": "Load All"
              },
              "Notify Failure": {
                "Type": "Task",
                "Resource": "arn:aws:states:::lambda:invoke",
                "Parameters": {
                  "FunctionName": "${notifierLambdaArn}",
                  "Payload": {
                    "message.$": "States.Format('S3 Path: {}, ETL timestamp: {}, Execution Id: {}, Error: {}', $.s3_path, $.etl_timestamp, $$.Execution.Id, $.error.Cause)",
                    "event": "ETL-PRICE_ZONE",
                    "additional_info_file_s3.$": "$.intermediate_s3_name",
                    "additional_info_file_key.$": "$.intermediate_directory_path",
                    "backup_bucket.$": "$.backup_bucket",
                    "backup_file_path.$" : "$.backup_file_path",
                    "etl_output_path_key.$": "$.etl_output_path_key",
                    "stepFunctionExecutionId.$": "$$.Execution.Id"
                  }
                },
                "End": true
              },
              "Load All": {
                "Type": "Map",
                "ItemsPath": "$.opcoList.Payload",
                "MaxConcurrency": 2,
                "Parameters": {
                  "id.$": "$$.Map.Item.Value",
                  "partitioned_files_key.$": "$.partitioned_files_key",
                  "intermediate_s3_name.$": "$.intermediate_s3_name",
                  "intermediate_directory_path.$": "$.intermediate_directory_path",
                  "etl_timestamp.$": "$.etl_timestamp"
                },
                "Iterator": {
                  "StartAt": "Load Job",
                  "States": {
                    "Load Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "${prizeZoneDataLoadJobName}",
                        "Arguments": {
                          "--opco_id.$": "$.id",
                          "--partitioned_files_key.$": "$.partitioned_files_key",
                          "--intermediate_s3_name.$": "$.intermediate_s3_name",
                          "--intermediate_directory_path.$": "$.intermediate_directory_path",
                          "--etl_timestamp.$": "$.etl_timestamp"
                        }
                      },
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "States.TaskFailed"
                          ],
                          "IntervalSeconds": 3,
                          "MaxAttempts": 2,
                          "BackoffRate": 10
                        }
                      ],
                      "Catch": [
                        {
                          "ErrorEquals": [
                            "States.TaskFailed"
                          ],
                          "ResultPath": "$.error",
                          "Next": "Notify Job Failure"
                        }
                      ],
                      "ResultPath": "$.loadJob",
                      "End": true
                    },
                    "Notify Job Failure": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::lambda:invoke",
                      "Parameters": {
                        "FunctionName": "${notifierLambdaArn}",
                        "Payload":{
                           "opco_id.$":"$.id",
                           "message.$":"$.error.Cause",
                           "event": "ETL-PRICE_ZONE",
                           "additional_info_file_s3.$": "$.intermediate_s3_name",
                           "etl_output_path_key.$": "$.etl_output_path_key",
                           "backup_bucket.$": "$.backup_bucket",
                           "backup_file_path.$" : "$.backup_file_path",
                           "stepFunctionExecutionId.$": "$$.Execution.Id"
                        }
                      },
                      "End": true
                    }
                  }
                },
                "ResultPath": "$.loadJobsResult",
                "Next": "Analyze load job statuses"
              },
              "Analyze load job statuses": {
                "Type": "Task",
                "Resource": "arn:aws:states:::lambda:invoke",
                "Parameters": {
                  "FunctionName": "${jobStatusAnalyzerLambdaArn}",
                  "Payload": {
                    "loadJobStatuses.$": "$.loadJobsResult"
                  }
                },
                "ResultPath": "$.loadJobStatuses",
                "Catch": [
                  {
                    "ErrorEquals": [
                      "States.TaskFailed"
                    ],
                    "ResultPath": "$.error",
                    "Next": "Notify Failure"
                  }
                ],
                "Next": "BackupOrNot"
              },
              "BackupOrNot": {
                "Comment": "Check on load job statuses and decide data backup",
                "Type": "Choice",
                "Choices": [
                  {
                    "Variable": "$.loadJobStatuses.Payload.isAllSuccess",
                    "BooleanEquals": true,
                    "Next": "Backup Data"
                  }
                ],
                "Default": "Skip Backup"
              },
              "Backup Data": {
                "Comment": "Backup data python shell job",
                "Type": "Task",
                "Resource": "arn:aws:states:::glue:startJobRun.sync",
                "Parameters": {
                  "JobName": "${prizeZoneDataBackupJobName}",
                  "Arguments": {
                    "--s3_input_bucket.$": "$.s3_input_bucket",
                    "--s3_input_file_key.$": "$.s3_input_file_key",
                    "--etl_timestamp.$": "$.etl_timestamp",
                    "--partitioned_files_key.$": "$.partitioned_files_key",
                    "--decompressed_file_path.$": "$.decompressed_file_path",
                    "--etl_output_path_key.$": "$.etl_output_path_key",
                    "--backup_file_path.$": "$.backup_file_path",
                    "--intermediate_directory_path.$": "$.intermediate_directory_path"
                  }
                },
                "ResultPath": "$.response",
                "Catch": [
                  {
                    "ErrorEquals": [
                      "States.TaskFailed"
                    ],
                    "ResultPath": "$.error",
                    "Next": "Notify Failure"
                  }
                ],
                "Next": "Notify ETL Success"
              },
              "Skip Backup": {
                "Type": "Succeed"
              },
              "Notify ETL Success": {
                "Type": "Task",
                "Resource": "arn:aws:states:::lambda:invoke",
                "Parameters": {
                  "FunctionName": "${notifierLambdaArn}",
                  "Payload": {
                    "event": "[ETL] - [Ref Price] [Price Zone Data]",
                    "backup_bucket.$": "$.backup_bucket",
                    "backup_file_path.$" : "$.backup_file_path",
                    "message.$": "States.Format('S3 Path: {}, ETL timestamp: {}, Execution Id: {}', $.s3_path, $.etl_timestamp, $$.Execution.Id)",
                    "additional_info_file_s3": "${backupS3Name}",
                    "etl_timestamp.$": "$.etl_timestamp",
                    "etl_output_path_key.$": "$.etl_output_path_key",
                    "status": "SUCCEEDED"
                  }
                },
                "End": true
              }
            }
          }
        - {
            prizeZoneDecompressJobName: !Ref PrizeZoneDecompressJob,
            prizeZoneTransformJobName: !Ref PrizeZoneTransformJob,
            prizeZoneDataLoadJobName: !Ref PrizeZoneDataLoadJob,
            prizeZoneDataBackupJobName: !Ref PrizeZoneDataBackupJob,
            opCoFilesFetchLambdaArn: !GetAtt OpCoFilesFetchLambda.Arn,
            jobStatusAnalyzerLambdaArn: !GetAtt PriceZoneJobStatusAnalyzerLambda.Arn,
            waitStatusAnalyzerLambdaArn: !GetAtt PriceZoneETLWaitStatusAnalyzerLambda.Arn,
            notifierLambdaArn: !Ref NotifierLambdaArn,
            metadataAggregator: !Sub 'CP-REF-PRICE-ETL-metadata-aggregator-${EnvironmentShort}',
            backupS3Name: !Ref ETLDataBackUpStorage,
            backupedAdditonalInfoFile: !Ref ETLDataBackUpStorage
        }
      LoggingConfiguration:
        Destinations:
          - CloudWatchLogsLogGroup:
              LogGroupArn: !GetAtt PriceZoneStepFunctionExecutionLogs.Arn
        IncludeExecutionData: TRUE
        Level: ALL
      RoleArn: !Ref StepFuctionExecutionRole
      Tags:
        - Key: Technical:ApplicationName
          Value: !Ref ApplicationName
        - Key: Technical:ApplicationID
          Value: !Ref ApplicationID
        - Key: Technical:PlatformOwner
          Value: !Ref Owner
        - Key: Technical:Environment
          Value: !FindInMap [ EnvMap, !Ref EnvironmentShort, name ]
        - Key: Support_Email
          Value: !Ref SupportEmail
        - Key: Approver
          Value: !Ref Approver
        - Key: PO_Number
          Value: !Ref PONumber
        - Key: Project_ID
          Value: !Ref ProjectID
        - Key: 2WTAGGER
          Value: !Ref 2WTAGGER
        - Key: Platform
          Value: !Ref Platform
        - Key: Component
          Value: !Ref Component
